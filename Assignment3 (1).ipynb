{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0687196-5d59-41da-ba92-470ce4a45ff0",
   "metadata": {},
   "source": [
    "1. Write a Python program to read a Hadoop configuration file and display the core components of Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889efd9c-b36d-432c-ac71-b2857a036310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Specify the path to the Hadoop configuration file\n",
    "hadoop_conf_file = '/path/to/hadoop/conf/hadoop-env.sh'\n",
    "\n",
    "# Create a configparser object\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read the Hadoop configuration file\n",
    "config.read(hadoop_conf_file)\n",
    "\n",
    "# Retrieve the core components from the configuration file\n",
    "core_components = config.get('core-site', 'fs.defaultFS')\n",
    "\n",
    "# Display the core components\n",
    "print(\"Core Components of Hadoop:\")\n",
    "print(core_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e88a6c-159c-4c04-95a8-46e63c6e31c5",
   "metadata": {},
   "source": [
    "\n",
    "2. Implement a Python function that calculates the total file size in a Hadoop Distributed File System (HDFS) directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a860d91-8c0c-4786-8576-6ec5bbfcf045",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install hdfs\n",
    "\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def calculate_total_file_size(hdfs_url, directory):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # Get the file status for the directory\n",
    "    file_status = client.list(directory, status=True)\n",
    "\n",
    "    # Initialize total file size to 0\n",
    "    total_size = 0\n",
    "\n",
    "    # Iterate over the file status\n",
    "    for file in file_status:\n",
    "        # Add the file size to the total size\n",
    "        total_size += file['length']\n",
    "\n",
    "    return total_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75047b1-6f42-4514-9e76-3103457a9e21",
   "metadata": {},
   "source": [
    "3. Create a Python program that extracts and displays the top N most frequent words from a large text file using the MapReduce approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2818b-c366-4e25-97cb-428c2cd1a1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import heapq\n",
    "\n",
    "class TopNWords(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner_count_words(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "\n",
    "    def reducer_find_top_n_words(self, _, word_counts):\n",
    "        top_n = heapq.nlargest(N, word_counts)\n",
    "        for count, word in top_n:\n",
    "            yield word, count\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   combiner=self.combiner_count_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_find_top_n_words)\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    N = 10  # Specify the value of N for the top N words\n",
    "    input_file = 'path/to/large_text_file.txt'  # Replace with the path to your large text file\n",
    "    mr_job = TopNWords(args=[input_file])\n",
    "    with mr_job.make_runner() as runner:\n",
    "        runner.run()\n",
    "        for word, count in mr_job.parse_output(runner.cat_output()):\n",
    "            print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76915e27-a775-4308-a003-d505f882e1aa",
   "metadata": {},
   "source": [
    "4. Write a Python script that checks the health status of the NameNode and DataNodes in a Hadoop cluster using Hadoop's REST API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d8ceb-6b13-456c-97a0-183cb627cd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Hadoop cluster configuration\n",
    "namenode_host = 'namenode_hostname'\n",
    "namenode_port = 50070\n",
    "datanode_port = 50075\n",
    "\n",
    "# Check NameNode health status\n",
    "namenode_url = f\"http://{namenode_host}:{namenode_port}/jmx?qry=Hadoop:service=NameNode,name=NameNodeStatus\"\n",
    "response = requests.get(namenode_url)\n",
    "namenode_status = response.json()['beans'][0]['State']\n",
    "\n",
    "# Check DataNode health status\n",
    "datanode_url = f\"http://{namenode_host}:{datanode_port}/jmx?qry=Hadoop:service=DataNode,name=DataNodeInfo\"\n",
    "response = requests.get(datanode_url)\n",
    "datanode_status = response.json()['beans'][0]['State']\n",
    "\n",
    "# Display the health status\n",
    "print(\"NameNode Status:\", namenode_status)\n",
    "print(\"DataNode Status:\", datanode_status)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4b679c-348b-49f9-914f-da748a67b653",
   "metadata": {},
   "source": [
    "5. Develop a Python program that lists all the files and directories in a specific HDFS path.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7dc662-bec5-4128-841c-d5666d317669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "\n",
    "def list_hdfs_path(hdfs_url, hdfs_path):\n",
    "    # Create an HDFS client\n",
    "    client = InsecureClient(hdfs_url)\n",
    "\n",
    "    # List the files and directories in the HDFS path\n",
    "    file_status = client.list(hdfs_path, status=True)\n",
    "\n",
    "    # Iterate over the file status\n",
    "    for file in file_status:\n",
    "        if file['type'] == 'DIRECTORY':\n",
    "            print(f\"[DIR] {file['path']}\")\n",
    "        else:\n",
    "            print(f\"[FILE] {file['path']}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hdfs_url = 'http://localhost:50070'  # Replace with the HDFS URL\n",
    "    hdfs_path = '/your/hdfs/path'  # Replace with the specific HDFS path\n",
    "    list_hdfs_path(hdfs_url, hdfs_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f0a69-7784-4b8c-9c74-06bf0376f583",
   "metadata": {},
   "source": [
    "6. Implement a Python program that analyzes the storage utilization of DataNodes in a Hadoop cluster and identifies the nodes with the highest and lowest storage capacities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a01729-841f-4219-b8b9-d8b0d07a863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def analyze_datanode_storage(hadoop_web_url):\n",
    "    # Retrieve DataNode information from Hadoop's REST API\n",
    "    url = f\"{hadoop_web_url}/jmx?qry=Hadoop:service=DataNode,name=FSDatasetState-*\"\n",
    "    response = requests.get(url)\n",
    "    data_nodes = response.json()['beans']\n",
    "\n",
    "    # Sort DataNodes based on storage capacity\n",
    "    data_nodes.sort(key=lambda x: x['Capacity'], reverse=True)\n",
    "\n",
    "    # Identify the node with the highest storage capacity\n",
    "    highest_capacity_node = data_nodes[0]\n",
    "\n",
    "    # Identify the node with the lowest storage capacity\n",
    "    lowest_capacity_node = data_nodes[-1]\n",
    "\n",
    "    # Print the results\n",
    "    print(\"DataNode with the highest storage capacity:\")\n",
    "    print(f\"Hostname: {highest_capacity_node['Host']}\")\n",
    "    print(f\"Storage Capacity: {highest_capacity_node['Capacity']} bytes\")\n",
    "\n",
    "    print(\"\\nDataNode with the lowest storage capacity:\")\n",
    "    print(f\"Hostname: {lowest_capacity_node['Host']}\")\n",
    "    print(f\"Storage Capacity: {lowest_capacity_node['Capacity']} bytes\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    hadoop_web_url = 'http://localhost:9870'  # Replace with the Hadoop Web URL\n",
    "    analyze_datanode_storage(hadoop_web_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2bf74-04d3-41e7-aefe-60a202f38ff4",
   "metadata": {},
   "source": [
    "7. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, monitor its progress, and retrieve the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f6a076-1ec6-498d-9aa3-781798a70803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, job_file):\n",
    "    # Submit the Hadoop job\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "    app_id = response.json()['application-id']\n",
    "\n",
    "    # Upload the job file\n",
    "    upload_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/upload\"\n",
    "    with open(job_file, 'rb') as file:\n",
    "        requests.post(upload_url, files={'job': file})\n",
    "\n",
    "    # Submit the job\n",
    "    submit_job_url = f\"{resource_manager_url}/ws/v1/cluster/apps\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"application-id\": app_id,\n",
    "        \"application-name\": \"Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": \"hadoop jar job.jar\"\n",
    "            }\n",
    "        },\n",
    "        \"application-type\": \"MAPREDUCE\"\n",
    "    }\n",
    "    requests.post(submit_job_url, headers=headers, json=data)\n",
    "\n",
    "    return app_id\n",
    "\n",
    "def monitor_job_progress(resource_manager_url, app_id):\n",
    "    # Monitor the job progress\n",
    "    while True:\n",
    "        status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}\"\n",
    "        response = requests.get(status_url)\n",
    "        app_status = response.json()['app']['state']\n",
    "\n",
    "        if app_status in {'FINISHED', 'FAILED', 'KILLED'}:\n",
    "            break\n",
    "\n",
    "        print(f\"Job status: {app_status}\")\n",
    "        time.sleep(5)\n",
    "\n",
    "def retrieve_job_output(resource_manager_url, app_id):\n",
    "    # Retrieve the job output\n",
    "    output_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/reports\"\n",
    "    response = requests.get(output_url)\n",
    "    output = response.json()['app']['appReport']['finalStatus']\n",
    "\n",
    "    print(\"Job output:\", output)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resource_manager_url = 'http://localhost:8088'  # Replace with the ResourceManager URL\n",
    "    job_file = 'path/to/hadoop/job.jar'  # Replace with the path to your Hadoop job file\n",
    "\n",
    "    app_id = submit_hadoop_job(resource_manager_url, job_file)\n",
    "    print(\"Job submitted. Application ID:\", app_id)\n",
    "\n",
    "    monitor_job_progress(resource_manager_url, app_id)\n",
    "    print(\"Job completed.\")\n",
    "\n",
    "    retrieve_job_output(resource_manager_url, app_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c9b53-ad84-47db-bb76-557b1d881004",
   "metadata": {},
   "source": [
    "8. Create a Python script that interacts with YARN's ResourceManager API to submit a Hadoop job, set resource requirements, and track resource usage during job execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fced69-2956-45f0-9671-cabd03c03e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def submit_hadoop_job(resource_manager_url, job_file, num_containers, container_memory, container_vcores):\n",
    "    # Submit the Hadoop job\n",
    "    submit_url = f\"{resource_manager_url}/ws/v1/cluster/apps/new-application\"\n",
    "    response = requests.post(submit_url)\n",
    "    app_id = response.json()['application-id']\n",
    "\n",
    "    # Upload the job file\n",
    "    upload_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/upload\"\n",
    "    with open(job_file, 'rb') as file:\n",
    "        requests.post(upload_url, files={'job': file})\n",
    "\n",
    "    # Set the resource requirements\n",
    "    set_resources_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"resource\": {\n",
    "            \"memory\": container_memory,\n",
    "            \"vCores\": container_vcores\n",
    "        },\n",
    "        \"application-id\": app_id\n",
    "    }\n",
    "    requests.put(set_resources_url, headers=headers, json=data)\n",
    "\n",
    "    # Submit the job\n",
    "    submit_job_url = f\"{resource_manager_url}/ws/v1/cluster/apps\"\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\n",
    "        \"application-id\": app_id,\n",
    "        \"application-name\": \"Hadoop Job\",\n",
    "        \"am-container-spec\": {\n",
    "            \"commands\": {\n",
    "                \"command\": \"hadoop jar job.jar\"\n",
    "            },\n",
    "            \"resource\": {\n",
    "                \"memory\": container_memory,\n",
    "                \"vCores\": container_vcores\n",
    "            },\n",
    "            \"localResources\": {\n",
    "                \"entry\": [\n",
    "                    {\n",
    "                        \"key\": \"job.jar\",\n",
    "                        \"value\": {\n",
    "                            \"resource\": f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/job\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        },\n",
    "        \"application-type\": \"MAPREDUCE\",\n",
    "        \"resource\": {\n",
    "            \"memory\": container_memory,\n",
    "            \"vCores\": container_vcores\n",
    "        }\n",
    "    }\n",
    "    requests.post(submit_job_url, headers=headers, json=data)\n",
    "\n",
    "    return app_id\n",
    "\n",
    "def monitor_job_progress(resource_manager_url, app_id):\n",
    "    # Monitor the job progress\n",
    "    while True:\n",
    "        status_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}\"\n",
    "        response = requests.get(status_url)\n",
    "        app_status = response.json()['app']['state']\n",
    "\n",
    "        if app_status in {'FINISHED', 'FAILED', 'KILLED'}:\n",
    "            break\n",
    "\n",
    "        resources_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/appattempts\"\n",
    "        response = requests.get(resources_url)\n",
    "        app_resources = response.json()['appAttempts']['appAttempt'][0]['resourceUsage']\n",
    "\n",
    "        memory_used = app_resources['memorySeconds']\n",
    "        vcores_used = app_resources['vcoreSeconds']\n",
    "\n",
    "        print(f\"Job status: {app_status}\")\n",
    "        print(f\"Memory Used: {memory_used} MB-seconds\")\n",
    "        print(f\"vCores Used: {vcores_used} vCore-seconds\")\n",
    "        time.sleep(5)\n",
    "\n",
    "    resources_url = f\"{resource_manager_url}/ws/v1/cluster/apps/{app_id}/appattempts\"\n",
    "    response = requests.get(resources_url)\n",
    "    app_resources = response.json()['appAttempts']['appAttempt'][0]['resourceUsage']\n",
    "\n",
    "    memory_used = app_resources['memorySeconds']\n",
    "    vcores_used = app_resources['vcoreSeconds']\n",
    "\n",
    "    print(f\"Final Memory Used: {memory_used} MB-seconds\")\n",
    "    print(f\"Final vCores Used: {vcores_used} vCore-seconds\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resource_manager_url = 'http://localhost:8088'  # Replace with the ResourceManager URL\n",
    "    job_file = 'path/to/hadoop/job.jar'  # Replace with the path to your Hadoop job file\n",
    "    num_containers = 2  # Specify the number of containers required\n",
    "    container_memory = 1024  # Specify the memory requirement per container in MB\n",
    "    container_vcores = 1  # Specify the vCores requirement per container\n",
    "\n",
    "    app_id = submit_hadoop_job(resource_manager_url, job_file, num_containers, container_memory, container_vcores)\n",
    "    print(\"Job submitted. Application ID:\", app_id)\n",
    "\n",
    "    monitor_job_progress(resource_manager_url, app_id)\n",
    "    print(\"Job completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5e824-8d06-4a5e-a932-64bee394ea5b",
   "metadata": {},
   "source": [
    "9. Write a Python program that compares the performance of a MapReduce job with different input split sizes, showcasing the impact on overall job execution time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f1f65-dc88-4573-b69b-94c5acad81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import time\n",
    "\n",
    "class WordCountJob(MRJob):\n",
    "    \n",
    "    def configure_args(self):\n",
    "        super(WordCountJob, self).configure_args()\n",
    "        self.add_passthru_arg(\n",
    "            '--input-split-size',\n",
    "            type=int,\n",
    "            default=64,\n",
    "            help='Size of each input split in MB (default: 64)'\n",
    "        )\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in line.split():\n",
    "            yield word.lower(), 1\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.mapper,\n",
    "                combiner=self.combiner,\n",
    "                reducer=self.reducer,\n",
    "                jobconf={\n",
    "                    'mapreduce.input.fileinputformat.split.maxsize': str(self.options.input_split_size * 1024 * 1024)\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Input file path\n",
    "    input_file = 'path/to/input/file.txt'\n",
    "\n",
    "    # List of input split sizes to compare\n",
    "    split_sizes = [32, 64, 128]\n",
    "\n",
    "    for split_size in split_sizes:\n",
    "        print(f\"Comparing input split size: {split_size} MB\")\n",
    "\n",
    "        # Start the timer\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run the WordCount job with the specified input split size\n",
    "        WordCountJob.run(args=[input_file, f'--input-split-size={split_size}'])\n",
    "\n",
    "        # Calculate and print the execution time\n",
    "        execution_time = time.time() - start_time\n",
    "        print(f\"Execution time: {execution_time} seconds\")\n",
    "        print(\"--------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
