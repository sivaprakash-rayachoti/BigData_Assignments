{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd85194d-43c2-404e-a0e3-8c1cc77e9bff",
   "metadata": {},
   "source": [
    "TOPIC: Data Warehousing Fundamentals\n",
    "   1. Design a data warehouse schema for a retail company that includes dimension tables for products, customers, and time. Implement the schema using a relational database management system (RDBMS) of your choice.\n",
    "   2. Create a fact table that captures sales data, including product ID, customer ID, date, and sales amount. Populate the fact table with sample data.\n",
    "   3. Write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cb1c1b-ce9a-4777-8326-51273bc71f1f",
   "metadata": {},
   "source": [
    "Product Dimension:\n",
    "- product_id (primary key)\n",
    "- product_name\n",
    "- category\n",
    "- brand\n",
    "- ...\n",
    "\n",
    "Customer Dimension:\n",
    "- customer_id (primary key)\n",
    "- customer_name\n",
    "- address\n",
    "- city\n",
    "- ...\n",
    "\n",
    "Time Dimension:\n",
    "- date_id (primary key)\n",
    "- date\n",
    "- year\n",
    "- month\n",
    "- quarter\n",
    "- ...\n",
    "\n",
    "Sales Fact Table:\n",
    "- sales_id (primary key)\n",
    "- product_id (foreign key referencing Product Dimension)\n",
    "- customer_id (foreign key referencing Customer Dimension)\n",
    "- date_id (foreign key referencing Time Dimension)\n",
    "- sales_amount\n",
    "- ...\n",
    "\n",
    "\n",
    "\n",
    "<!-- Creating a Fact Table and Populating Sample Data:\n",
    "Once you have the schema in place, you can create a fact table to capture sales data and populate it with sample data. Here's an example of how you can create and populate the fact table using SQL: -->\n",
    "\n",
    "\n",
    "-- Create the Sales Fact Table\n",
    "CREATE TABLE sales_fact (\n",
    "  sales_id INT PRIMARY KEY,\n",
    "  product_id INT,\n",
    "  customer_id INT,\n",
    "  date_id INT,\n",
    "  sales_amount DECIMAL(10, 2),\n",
    "  -- Add more columns as needed\n",
    "  FOREIGN KEY (product_id) REFERENCES product_dim(product_id),\n",
    "  FOREIGN KEY (customer_id) REFERENCES customer_dim(customer_id),\n",
    "  FOREIGN KEY (date_id) REFERENCES time_dim(date_id)\n",
    ");\n",
    "\n",
    "-- Populate the Sales Fact Table with Sample Data\n",
    "INSERT INTO sales_fact (sales_id, product_id, customer_id, date_id, sales_amount)\n",
    "VALUES\n",
    "  (1, 1, 1, 1, 100.00),\n",
    "  (2, 2, 1, 1, 50.00),\n",
    "  (3, 3, 2, 2, 75.00),\n",
    "  -- Add more sample data as needed\n",
    ";\n",
    "\n",
    "\n",
    "Writing SQL Queries to Retrieve Sales Data:\n",
    "Once the data warehouse schema is set up, you can write SQL queries to retrieve sales data from the data warehouse, including aggregations and filtering based on different dimensions. Here are a few examples:\n",
    "Retrieve total sales amount for a specific date:\n",
    "\n",
    "SELECT\n",
    "  t.date,\n",
    "  SUM(f.sales_amount) AS total_sales_amount\n",
    "FROM\n",
    "  sales_fact f\n",
    "JOIN\n",
    "  time_dim t ON f.date_id = t.date_id\n",
    "WHERE\n",
    "  t.date = '2023-01-01'\n",
    "GROUP BY\n",
    "  t.date;\n",
    "  \n",
    "Retrieve sales amount by product category:\n",
    "SELECT\n",
    "  p.category,\n",
    "  SUM(f.sales_amount) AS total_sales_amount\n",
    "FROM\n",
    "  sales_fact f\n",
    "JOIN\n",
    "  product_dim p ON f.product_id = p.product_id\n",
    "GROUP BY\n",
    "  p.category;\n",
    "\n",
    "\n",
    "\n",
    "Retrieve sales amount by customer and month:\n",
    "SELECT\n",
    "  c.customer_name,\n",
    "  t.month,\n",
    "  SUM(f.sales_amount) AS total_sales_amount\n",
    "FROM\n",
    "  sales_fact f\n",
    "JOIN\n",
    "  customer_dim c ON f.customer_id = c.customer_id\n",
    "JOIN\n",
    "  time_dim t ON f.date_id = t.date_id\n",
    "GROUP BY\n",
    "  c.customer_name,\n",
    "  t.month;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5ee04-6bc4-4a1a-8c6a-b929d257c820",
   "metadata": {},
   "source": [
    "\n",
    "TOPIC: ETL and Data Integration\n",
    "  1. Design an ETL process using a programming language (e.g., Python) to extract data from a source system (e.g., CSV files), transform it by applying certain business rules or calculations, and load it into a data warehouse.\n",
    "  2. Implement the ETL process by writing code that performs the extraction, transformation, and loading steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc1df5a-7660-4aa2-9539-c18b48b2548a",
   "metadata": {},
   "source": [
    "\n",
    "<!-- Designing an ETL Process: -->\n",
    "\n",
    "When designing an ETL process, the key steps involve extracting data from a source system, transforming it according to specific business rules or calculations, and loading it into a data warehouse. Here's an example of how you can design an ETL process:\n",
    "Extraction: Determine the source of the data, such as CSV files, databases, APIs, or other data sources. Identify the data to be extracted and the methods or tools required to extract the data effectively.\n",
    "\n",
    "Transformation: Define the transformations needed to convert the extracted data into the desired format for the data warehouse. This may include data cleansing, validation, filtering, aggregation, or applying business rules or calculations.\n",
    "\n",
    "Loading: Plan the loading process, including the target schema and data warehouse structure. Determine the loading strategy, such as batch loading or incremental loading, and consider any dependencies or constraints in the data warehouse.\n",
    "\n",
    "# Implementing the ETL Process in Pytho\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Extraction: Read data from CSV files\n",
    "def extract_data(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "# Transformation: Apply business rules or calculations\n",
    "def transform_data(data):\n",
    "    # Apply transformations to the data\n",
    "    transformed_data = data.copy()\n",
    "    transformed_data['sales_amount'] = transformed_data['quantity'] * transformed_data['unit_price']\n",
    "    # Add more transformations as needed\n",
    "    return transformed_data\n",
    "\n",
    "# Loading: Load transformed data into the data warehouse\n",
    "def load_data(data, target_table):\n",
    "    # Load data into the target table (data warehouse)\n",
    "    data.to_csv(target_table, index=False)\n",
    "    print(\"Data loaded successfully.\")\n",
    "\n",
    "# Main ETL process\n",
    "def etl_process(file_path, target_table):\n",
    "    # Extract data from source\n",
    "    extracted_data = extract_data(file_path)\n",
    "\n",
    "    # Transform data\n",
    "    transformed_data = transform_data(extracted_data)\n",
    "\n",
    "    # Load transformed data into the data warehouse\n",
    "    load_data(transformed_data, target_table)\n",
    "\n",
    "# Example usage\n",
    "etl_process('source_data.csv', 'target_table.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3666f8ed-6609-4ceb-8c25-9e1db4747bb8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "TOPIC: Dimensional Modeling and Schemas\n",
    "   1. Design a star schema for a university database, including a fact table for student enrollments and dimension tables for students, courses, and time. Implement the schema using a database of your choice.\n",
    "   2. Write SQL queries to retrieve data from the star schema, including aggregations and joins between the fact table and dimension tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf2f905-1229-471c-a94d-f8193e23f3e5",
   "metadata": {},
   "source": [
    "\n",
    "Designing a Star Schema for a University Database:\n",
    "A star schema is a popular dimensional modeling technique for organizing data in a data warehouse. Here's an example of a star schema design for a university database:\n",
    "\n",
    "Student Dimension:\n",
    "- student_id (primary key)\n",
    "- student_name\n",
    "- student_major\n",
    "- ...\n",
    "\n",
    "Course Dimension:\n",
    "- course_id (primary key)\n",
    "- course_name\n",
    "- course_department\n",
    "- ...\n",
    "\n",
    "Time Dimension:\n",
    "- time_id (primary key)\n",
    "- date\n",
    "- semester\n",
    "- ...\n",
    "\n",
    "Enrollment Fact Table:\n",
    "- enrollment_id (primary key)\n",
    "- student_id (foreign key referencing Student Dimension)\n",
    "- course_id (foreign key referencing Course Dimension)\n",
    "- time_id (foreign key referencing Time Dimension)\n",
    "- grade\n",
    "- ...\n",
    "\n",
    "\n",
    "Retrieve the total number of enrollments by semester:\n",
    "SELECT\n",
    "  t.semester,\n",
    "  COUNT(*) AS total_enrollments\n",
    "FROM\n",
    "  enrollment_fact e\n",
    "JOIN\n",
    "  time_dim t ON e.time_id = t.time_id\n",
    "GROUP BY\n",
    "  t.semester;\n",
    "\n",
    "\n",
    "\n",
    "Retrieve the average grade by course department:\n",
    "SELECT\n",
    "  c.course_department,\n",
    "  AVG(e.grade) AS average_grade\n",
    "FROM\n",
    "  enrollment_fact e\n",
    "JOIN\n",
    "  course_dim c ON e.course_id = c.course_id\n",
    "GROUP BY\n",
    "  c.course_department;\n",
    "\n",
    "\n",
    "Retrieve the student name, course name, and grade for a specific enrollment:\n",
    "SELECT\n",
    "  s.student_name,\n",
    "  c.course_name,\n",
    "  e.grade\n",
    "FROM\n",
    "  enrollment_fact e\n",
    "JOIN\n",
    "  student_dim s ON e.student_id = s.student_id\n",
    "JOIN\n",
    "  course_dim c ON e.course_id = c.course_id\n",
    "WHERE\n",
    "  e.enrollment_id = 1; -- Replace with the desired enrollment ID\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c389a4d6-4160-412c-9936-b9bc40b3ebfb",
   "metadata": {},
   "source": [
    "\n",
    "TOPIC: Performance Optimization and Querying\n",
    "    1. Scenario: You need to improve the performance of your data loading process in the data warehouse. Write a Python script that implements the following optimizations:\n",
    "Utilize batch processing techniques to load data in bulk instead of individual row insertion.\n",
    "      b)  Implement multi-threading or multiprocessing to parallelize the data loading process.\n",
    "      c)  Measure the time taken to load a specific amount of data before and after implementing these optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69be0ef-8611-4eb9-92df-ab88b666e09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "# Function to load data in batches\n",
    "def load_data_batch(data_batch):\n",
    "    # Code to load the data batch into the data warehouse\n",
    "    # Replace this with your actual data loading logic\n",
    "\n",
    "# Function to measure time taken to load data\n",
    "def measure_loading_time(data):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load data using batch processing\n",
    "    batch_size = 1000\n",
    "    data_batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "    for data_batch in data_batches:\n",
    "        load_data_batch(data_batch)\n",
    "\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Time taken to load data: {loading_time} seconds\")\n",
    "\n",
    "# Function to load data using multi-threading\n",
    "def load_data_threading(data):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define the number of threads to use\n",
    "    num_threads = 4\n",
    "\n",
    "    # Divide the data equally among the threads\n",
    "    data_per_thread = len(data) // num_threads\n",
    "\n",
    "    # Create and start the threads\n",
    "    threads = []\n",
    "    for i in range(num_threads):\n",
    "        start_index = i * data_per_thread\n",
    "        end_index = start_index + data_per_thread\n",
    "        thread = threading.Thread(target=load_data_batch, args=(data[start_index:end_index],))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "\n",
    "    # Wait for all threads to complete\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Time taken to load data (with multi-threading): {loading_time} seconds\")\n",
    "\n",
    "# Function to load data using multiprocessing\n",
    "def load_data_multiprocessing(data):\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define the number of processes to use\n",
    "    num_processes = 4\n",
    "\n",
    "    # Divide the data equally among the processes\n",
    "    data_per_process = len(data) // num_processes\n",
    "\n",
    "    # Create and start the processes\n",
    "    processes = []\n",
    "    for i in range(num_processes):\n",
    "        start_index = i * data_per_process\n",
    "        end_index = start_index + data_per_process\n",
    "        process = multiprocessing.Process(target=load_data_batch, args=(data[start_index:end_index],))\n",
    "        processes.append(process)\n",
    "        process.start()\n",
    "\n",
    "    # Wait for all processes to complete\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "\n",
    "    end_time = time.time()\n",
    "    loading_time = end_time - start_time\n",
    "    print(f\"Time taken to load data (with multiprocessing): {loading_time} seconds\")\n",
    "\n",
    "# Sample data to be loaded\n",
    "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  # Replace with your actual data\n",
    "\n",
    "# Measure time taken to load data without optimizations\n",
    "measure_loading_time(data)\n",
    "\n",
    "# Measure time taken to load data with multi-threading\n",
    "load_data_threading(data)\n",
    "\n",
    "# Measure time taken to load data with multiprocessing\n",
    "load_data_multiprocessing(data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
